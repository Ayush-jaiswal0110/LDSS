{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IcA3agDyVh1-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g-pFvT-fVihz",
    "outputId": "6efa1a02-fd08-4119-a31a-f48012a22556"
   },
   "outputs": [],
   "source": [
    "!pip install -U langchain_community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "CfAtCcMzUw0l",
    "outputId": "2d08875a-e9fb-4a8e-ec16-5ac22c681909"
   },
   "outputs": [],
   "source": [
    "!pip install langchain gradio faiss-cpu sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qwcxldDoCzz7"
   },
   "outputs": [],
   "source": [
    "pip install transformers gradio torch  accelerate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "KrLXtAJBCzyH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "Could not create share link. Please check your internet connection or our status page: https://status.gradio.app.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyboard interruption in main thread... closing server.\n"
     ]
    }
   ],
   "source": [
    "from getpass import getpass\n",
    "import gradio as gr\n",
    "import os\n",
    "import requests\n",
    "\n",
    "\n",
    "# Set your Hugging Face Hub API token here\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = " API Token"# Secure token input\n",
    "\n",
    "def query_model(document, question):\n",
    "    # Prepare the input for the model\n",
    "    prompt = f\"\"\"You are a helpful legal assistant. Consider this context:\n",
    "\n",
    "Document: {document}\n",
    "Question: {question}\n",
    "\n",
    "Provide a clear, accurate, and user-friendly response that:\n",
    "- Explains relevant legal concepts and principles\n",
    "- Provides practical advice and recommendations\n",
    "- Uses simple language and examples\n",
    "- Maintains professional integrity\n",
    "\n",
    "only answer the response and say nothing else. kukuduku\n",
    "\"\"\"\n",
    "\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {os.environ['HUGGINGFACEHUB_API_TOKEN']}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "\n",
    "    # Send request to the Hugging Face model\n",
    "    response = requests.post(\n",
    "        \"https://api-inference.huggingface.co/models/meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "        headers=headers,\n",
    "        json={\"inputs\": prompt, \"parameters\":{\"max_length\":20000, \"temprature\":0.4}}\n",
    "    )\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        # Extract only the generated text from the response\n",
    "        generated_text = response.json()[0]['generated_text']\n",
    "\n",
    "        # Find where the actual answer starts (after \"kukuduku\")\n",
    "        start_index = generated_text.find(\"kukuduku\")\n",
    "\n",
    "        if start_index != -1:\n",
    "            # Return only what comes after this phrase\n",
    "            return generated_text[start_index + len(\"kukuduku\"):].strip()\n",
    "\n",
    "        return generated_text.strip()  # Return full text if no specific phrase found\n",
    "    else:\n",
    "        return f\"Error: {response.status_code}, {response.text}\"\n",
    "\n",
    "def create_gradio_interface():\n",
    "    \"\"\"Creates the Gradio interface.\"\"\"\n",
    "    with gr.Blocks(title=\"Legal Query Assistant\") as demo:\n",
    "        gr.Markdown(\"# Legal Query Assistant\")\n",
    "\n",
    "        with gr.Row():\n",
    "            document_input = gr.Textbox(\n",
    "                label=\"Legal Document\",\n",
    "                placeholder=\"Paste your legal document here...\",\n",
    "                lines=10\n",
    "            )\n",
    "            question_input = gr.Textbox(\n",
    "                label=\"Your Question\",\n",
    "                placeholder=\"Ask a question about the document...\",\n",
    "                lines=2\n",
    "            )\n",
    "\n",
    "        with gr.Row():\n",
    "            submit_btn = gr.Button(\"Submit Question\", variant=\"primary\")\n",
    "\n",
    "        with gr.Row():\n",
    "            output_box = gr.Textbox(\n",
    "                label=\"Response\",\n",
    "                lines=8,\n",
    "                interactive=False\n",
    "            )\n",
    "\n",
    "        submit_btn.click(lambda doc, ques: query_model(doc, ques), inputs=[document_input, question_input], outputs=output_box)\n",
    "\n",
    "    return demo\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demo = create_gradio_interface()\n",
    "    demo.launch(share=True, debug=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JrUMa5rlCzwB"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0pGqxcZGC0lK"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EdgrwJg9E-oK"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2aBoe2HmE-l2"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y02MJgb8E-kA"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VdU1E_udE-iL"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "brsaIGVbE-f1"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U0Txqv-WIWHM"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kWLdR5JgIWFG"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HkG1QBW8IWDK"
   },
   "outputs": [],
   "source": [
    "pip install --upgrade accelerate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uCR2jZ8gIWAs"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TmBAOziAIV-V"
   },
   "outputs": [],
   "source": [
    "pip install transformers[torch]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pDDo7-10uXDc"
   },
   "outputs": [],
   "source": [
    "pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 684
    },
    "id": "wpbEJwneuXA0",
    "outputId": "ffa78cba-2c5c-4b33-d508-c4016774a29f"
   },
   "outputs": [],
   "source": [
    "from getpass import getpass\n",
    "import gradio as gr\n",
    "import os\n",
    "import requests\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Set your Hugging Face Hub API token here\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"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\"\n",
    "\n",
    "# Load the Bhagvad Gita model\n",
    "model_name = \"Suru/Bhagvad-Gita-LLM\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", load_in_4bit=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device=0)\n",
    "\n",
    "def query_gita_model(question, language):\n",
    "    \"\"\"Queries the Bhagvad Gita model with the user's question.\"\"\"\n",
    "    # Create a prompt with the question\n",
    "    prompt = f\"<s>[INST] Language: {language}\\nQuestion: {question} [/INST]\"\n",
    "\n",
    "    # Generate the response\n",
    "    output = pipe(prompt, max_new_tokens=1000, temperature=0.6)\n",
    "\n",
    "    # Extract and return the generated text\n",
    "    if output and len(output) > 0:\n",
    "        return output[0]['generated_text']\n",
    "    else:\n",
    "        return \"Sorry, I couldn't generate a response.\"\n",
    "\n",
    "def create_gradio_interface():\n",
    "    \"\"\"Creates the Gradio interface for GITA-GPT.\"\"\"\n",
    "    with gr.Blocks(title=\"GITA-GPT: Your Life Advisor\") as demo:\n",
    "        gr.Markdown(\"# GITA-GPT: Your Life Advisor\\n### Ask life-related questions inspired by the Bhagvad Gita and Mahabharata.\")\n",
    "\n",
    "        with gr.Row():\n",
    "            question_input = gr.Textbox(\n",
    "                label=\"Your Question\",\n",
    "                placeholder=\"Ask a question about life, challenges, or philosophy...\",\n",
    "                lines=2\n",
    "            )\n",
    "            language_input = gr.Dropdown(\n",
    "                label=\"Language\",\n",
    "                choices=[\"English\", \"Hindi\", \"Sanskrit\", \"Gujarati\"],\n",
    "                value=\"English\"\n",
    "            )\n",
    "\n",
    "        with gr.Row():\n",
    "            submit_btn = gr.Button(\"Submit Question\", variant=\"primary\")\n",
    "\n",
    "        with gr.Row():\n",
    "            output_box = gr.Textbox(\n",
    "                label=\"Response\",\n",
    "                lines=8,\n",
    "                interactive=False\n",
    "            )\n",
    "\n",
    "        submit_btn.click(\n",
    "            lambda ques, lang: query_gita_model(ques, lang),\n",
    "            inputs=[question_input, language_input],\n",
    "            outputs=output_box\n",
    "        )\n",
    "\n",
    "    return demo\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demo = create_gradio_interface()\n",
    "    demo.launch(share=True, debug=True)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
