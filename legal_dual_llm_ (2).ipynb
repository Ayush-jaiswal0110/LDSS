{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uOKS8KYX6Bn9"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yzhZEVx36Bl0"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "86QDn5QN6BjY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in c:\\users\\ayush\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (24.3.1)Note: you may need to restart the kernel to use updated packages.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pip install  --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "shiBSmRX6Bgi"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install gradio langchain requests orjson --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface_hub in c:\\users\\ayush\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.26.2)\n",
      "Collecting huggingface_hub\n",
      "  Downloading huggingface_hub-0.26.5-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\ayush\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface_hub) (3.13.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\ayush\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface_hub) (2024.3.1)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\ayush\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface_hub) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\ayush\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface_hub) (6.0.1)\n",
      "Requirement already satisfied: requests in c:\\users\\ayush\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface_hub) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\ayush\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface_hub) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\ayush\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface_hub) (4.11.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\ayush\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tqdm>=4.42.1->huggingface_hub) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ayush\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->huggingface_hub) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ayush\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->huggingface_hub) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ayush\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->huggingface_hub) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ayush\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->huggingface_hub) (2024.2.2)\n",
      "Downloading huggingface_hub-0.26.5-py3-none-any.whl (447 kB)\n",
      "Installing collected packages: huggingface_hub\n",
      "  Attempting uninstall: huggingface_hub\n",
      "    Found existing installation: huggingface-hub 0.26.2\n",
      "    Uninstalling huggingface-hub-0.26.2:\n",
      "      Successfully uninstalled huggingface-hub-0.26.2\n",
      "Successfully installed huggingface_hub-0.26.5\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zgbkjtvn6Bay"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Q1PgxlOW4h_l"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "Could not create share link. Please check your internet connection or our status page: https://status.gradio.app.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ayush\\AppData\\Local\\Temp\\ipykernel_15524\\1149555555.py:65: LangChainDeprecationWarning: The method `BaseLLM.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response = self.chat_llm(prompt_text)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyboard interruption in main thread... closing server.\n"
     ]
    }
   ],
   "source": [
    "from getpass import getpass\n",
    "from langchain.llms import HuggingFaceHub\n",
    "from langchain.prompts import PromptTemplate\n",
    "import gradio as gr\n",
    "import os\n",
    "\n",
    "# Set your Hugging Face Hub API token here\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] =\"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\"\n",
    "class DualLLMLegalAnalysis:\n",
    "    def __init__(self):\n",
    "        # Initialize the LLMs\n",
    "        self.legal_llm = HuggingFaceHub(\n",
    "            repo_id=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "            model_kwargs={\"temperature\": 0.3, \"max_length\": 800}\n",
    "        )\n",
    "\n",
    "        self.chat_llm = HuggingFaceHub(\n",
    "            repo_id=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "            model_kwargs={\"temperature\": 0.7, \"max_length\": 800}\n",
    "        )\n",
    "\n",
    "        # Create prompt templates\n",
    "        self.legal_qa_prompt = PromptTemplate(\n",
    "            input_variables=[\"document\", \"question\"],\n",
    "            template=\"\"\"You are a legal expert. Review this legal context and question:\n",
    "\n",
    "Document: {document}\n",
    "\n",
    "Legal Question: {question}\n",
    "\n",
    "Provide a professional legal analysis focusing on:\n",
    "- Relevant legal principles\n",
    "- Applicable precedents\n",
    "- Potential implications\n",
    "- Professional recommendations\n",
    "\n",
    "Analysis: kukuduku\"\"\"\n",
    "        )\n",
    "\n",
    "        self.chat_qa_prompt = PromptTemplate(\n",
    "            input_variables=[\"document\", \"question\"],\n",
    "            template=\"\"\"You are a helpful assistant explaining legal concepts. Consider this context:\n",
    "\n",
    "Document: {document}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Provide a clear, user-friendly explanation that:\n",
    "- Uses simple language\n",
    "- Gives practical examples\n",
    "- Explains complex terms\n",
    "- Maintains accuracy while being accessible\n",
    "- also explains the IPC section defination \n",
    "\n",
    "Response: kukuduku\"\"\"\n",
    "        )\n",
    "\n",
    "    def query_legal(self, document: str, question: str) -> str:\n",
    "        prompt_text = self.legal_qa_prompt.format(document=document, question=question)\n",
    "        response = self.legal_llm(prompt_text)\n",
    "        return self.extract_response(response)\n",
    "\n",
    "    def query_chat(self, document: str, question: str) -> str:\n",
    "        prompt_text = self.chat_qa_prompt.format(document=document, question=question)\n",
    "        response = self.chat_llm(prompt_text)\n",
    "        return self.extract_response(response)\n",
    "\n",
    "    def extract_response(self, response: str) -> str:\n",
    "        # Extract only the relevant part of the response after \"kukuduku\"\n",
    "        start_index = response.find(\"kukuduku\")\n",
    "        if start_index != -1:\n",
    "            return response[start_index + len(\"kukuduku\"):].strip()\n",
    "\n",
    "        return response.strip()  # Return full text if no specific phrase found\n",
    "\n",
    "def create_gradio_interface(agent: DualLLMLegalAnalysis):\n",
    "    \"\"\"Creates the Gradio interface.\"\"\"\n",
    "\n",
    "    def submit_query(document: str, question: str, mode: str) -> str:\n",
    "        if mode == \"legal\":\n",
    "            return agent.query_legal(document, question)\n",
    "        else:\n",
    "            return agent.query_chat(document, question)\n",
    "\n",
    "    with gr.Blocks(title=\"Legal Query Assistant\") as demo:\n",
    "        gr.Markdown(\"# Legal Query Assistant\")\n",
    "\n",
    "        with gr.Row():\n",
    "            document_input = gr.Textbox(\n",
    "                label=\"Legal Document\",\n",
    "                placeholder=\"Paste your legal document here...\",\n",
    "                lines=10\n",
    "            )\n",
    "            question_input = gr.Textbox(\n",
    "                label=\"Your Question\",\n",
    "                placeholder=\"Ask a question about the document...\",\n",
    "                lines=2\n",
    "            )\n",
    "\n",
    "        with gr.Row():\n",
    "            mode_select = gr.Radio(\n",
    "                choices=[\"legal\", \"simple\"],\n",
    "                label=\"Response Mode\",\n",
    "                value=\"simple\",\n",
    "                info=\"Choose between professional legal analysis or simplified explanations\"\n",
    "            )\n",
    "            submit_btn = gr.Button(\"Submit Question\", variant=\"primary\")\n",
    "\n",
    "        with gr.Row():\n",
    "            output_box = gr.Textbox(\n",
    "                label=\"Response\",\n",
    "                lines=8,\n",
    "                interactive=False\n",
    "            )\n",
    "\n",
    "        submit_btn.click(submit_query, inputs=[document_input, question_input, mode_select], outputs=output_box)\n",
    "\n",
    "    return demo\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    agent = DualLLMLegalAnalysis()\n",
    "    demo = create_gradio_interface(agent)\n",
    "    demo.launch(share=True, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T_fFwBuw4iCb"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AuTJwIeJ4iEk"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
